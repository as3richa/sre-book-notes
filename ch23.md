---
title: "Managing Distributed State: Distributed Consensus for Reliability"
authors: ["Laura Nolan"]
---

CAP theorem: a distributed system cannot simultaneously be consistent across at all nodes, available at every node, and tolerant to network partitions.

ACID: atomicity, consistency, isolation, durability. BASE: basically available, soft state, eventual consistency. STONITH: (in the event of a communications failure) shoot the other node in the head.

When dealing with distributed software systems, we are interested in asynchronous distributed concensus, a model in which there are potential unbounded delays in message passing. Compare synchronous consensus, e.g. for real-time systems, in which message passing is guaranteed to meet specific time bounds.

Crash-fail vs. crash-recover: a dead node stays dead, or a dead node may re-join the system.

Byzantine vs. non-Byzantine failures: everyone implements the protocol honestly and correct, or some node behaves incorrect due to a bug or malicious activity.

Solving the asynchronous distributed consensus problem in bounded time is impossible. FLP impossibility result: no asynchronous distributed consensus algorithm can guarantee progress over an unreliable network.

Paxos operates as a series of proposals, which may or may not be accepted by a majority of processes. Each proposal has a sequence number imposing a strict ordering.

In the first phase of the protocol, a proposer sends a sequence number to its peers; each peer accepts the proposal only if it has not observed a proposal with a higher sequence number. Propsals may be retried with a higher sequence number as necessary.

Different nodes must draw their sequence numbers from disjoint sets, e.g. by including their hostnamee.

If a proposal is accepted by the majority of the nodes, it can commit the proposal by sending a commit message.

Chubby authors: by implementing a distributed consensus protocol as a service rather than a library, application developers need not deploy their systems in a way compatible with a conensus service (i.e. they don't need to worry about the right number of replicas, group membership, etc.).

Replicated state machine: a system that executes the same operations in the same order on several processes. Operations against an RSM are globally ordered through a consensus algorithm.

Atomic broadcast: messages are received reliably and in the same order by all participants.

Multi-Paxos: under normal operation, the first phase must only be executed once, and subequent proposals can simply be sent directly without sending the sequence number. If another node attempts to issue a proposal this can cause a dueling proposers situation in which proposers repeatedly interrupt one another.

In most consensus protocols, performing a strongly consistent read requires either an operation against a quorum, or a stable leader guaranteed to have seen all recent state-changing operations.

Quorum leasing: a lease against some subset of the state is granted to a read quorum of replicas; any operation that changes the state of that subset must be acknowledged by all replicas in the read quorum. Consistent reads can then be made against any node of the read quorum.

Regional proxies can be used to reduce the overhead of keeping connections open at the application level, and to encapsulate sharding or load balancing.

Fast Paxos: rather than sending messages through a single leader, a client attempts to send proposals to the entire cluster in parallel. Not necessarily faster; the cluster may have much faster links between the nodes, and due to tail latencies the overall latency of sending several messages over slow links may be inferior.

Pipelining allows multiple proposals to be in-flight at once.

A larger cluster of nodes allows for a higher overall throughput, because a quorum can be achieved with only the fastest nodes in the cluster.

Failure domain: a set of components of a system that can become unavailable due to a single failure (e.g. racks served by the same power supply, several DCs in the same region that could be affeced by a single natural disaster). Geographic distance between replicas increases latency, but also increases the size of failure that the system can tolerate.

Colocating leader processes in the same datacenter (when replicas are in other datacenters) causes uneven utilization of network bandwidth, because leaders tend to produce a much larger quantity of outgoing traffic.

Hierarchical quorum: quorum is determined by an agreement between several groups of nodes, with each group reaching agreement within itself by internal quorum (e.g. 3 groups of 3 nodes).
