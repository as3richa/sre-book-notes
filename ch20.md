---
title: "Load Balancing in the Datacenter"
authors: ["Alejandro Forero Cuervo"]
---

Assume there is a stream of queries arriving to the datacenter at a rate that doesn't exceed capacity, each query being directed towards a particular service. Queries are routed by client tasks to a set of backend tasks.

Ideally, load is spread perfectly such that the least and most loaded backend tasks for a given service experience the same level of load.

Robust approach to unhealthy tasks: lame duck state. A task is either healthy, refusing connections, or a lame duck. In the third case, the backend tasks is listening and is able to serve requests, but has explicitly requested that the client cease to send requests. This allows for e.g. clean shutdowns.

Client tasks maintain long-lived connections to backend tasks for the duration of the client's lifetime, because establishing a connection for each incoming request is costly.

In order to avoid the CPU and memory overhead (e.g. for health checks) of having many outgoing connections from a single client, subsetting is used to limit the set of backends that a given client interacts with.

Subset selection must handle changes in the number of client or backend processes while minimizing connection churn.

Random subsetting (e.g. each client selects a subset independently and at random) has low connection churn, but performs poorly in practice (in terms of the difference in load between the most- and least-loaded backends).

A better solution is deterministic subsetting: clients select consecutive slices of the repeatedly-shuffled list of all backends.

Simple round-robin load balancing on a per-client basis yields of to a 2x discrepency of load between backend tasks. This is induced by small subsetting, varying query costs, machine diversity, and unpredictable performance factors (e.g. other processes competing for resources on the same machine, JIT warmup).

Least-loaded round robin (by number of ongoing requests) has some pitfalls. If the backend is returning errors, then it may erroneously appear to have a very low load; this can be mitigated by counting recent errors as ongoing requests. Also, the number of requests may be a poor proxy for the actual load on a given backend, because ongoing requests may be blocked on a computationally inexpensive task (e.g. network I/O). Finally, the number of active requests as seen by one client doesn't account for the active requests on the same backend originating from another client. In practice, LLRR is approximately as performant as simple RR.

Weighted round robin improves on SRR/LLRR by incorporating information from the backend into the decision-making process. Clients maintain a "capability" score for each backend in its subset; requests are distributed in round-robin fashion, but requests are distributed proportional to capability. Backends return statistics on queries, errors, and utilization in every response (incl. health checks), and clients adjust capability scores periodically to pick healthier backend tasks.
