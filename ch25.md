---
title: "Data Processing Pipelines"
authors: ["Dan Dennison"]
---

The periodic pipeline model is fragile; on initial deployment, when sizing parameters, etc., are carefully tuned, performance is reliable, but organic growth causes degradation (e.g. resource exhaustion, missed deadlines).

At Google, periodic pipelines are scheduled as lower-priority batch jobs, often in the "gaps" left on machines by higher-priority web services. When cluster load is high, batch jobs are risk of preemption.

System prevalence pattern: all application state is stored in memory, with changes synchronously journalled to disk.
