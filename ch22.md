---
title: "Addressing Cascading Failures"
authors: ["Mike Ulrich"]
---

Cascading failures occur when one task or service becomes unhealthy, which causes another dependent service or task to become unhealthy, and so on.

E.g. a poorly tuned GC on a frontend application causes an unexpectedly high level of CPU consumption, which results in higher request latency and more concurrent requests, which induces memory pressure, which reduces caching efficacy, which causes a backend service to receive requests in excess of its capacity, at which point the backend service fails. It's not necessarily obvious mid-outage how the problem originated.

If a single overloaded server crashes, other servers running the same service take on additional load, which can lead to further crashes and further overloading.

Strategies for load shedding: drop all traffic above 10 RPS; drop all requests that aren't from users who have recently interacted with the service; drop requests for low-priority tasks.

Systems with fairly steady rates of traffic should have small queues relative to the number of serving threads. For systems with bursty traffic, a queue size should be based on the current number of threads in use, the processing time for each request, and the size/freuqency of bursts.

LIFO queueing can be better in the case that a request that has been queued for a long time is no longer worth processing (e.g. a search query that has been queued for 30 seconds). See also the controlled delay algorithm.

When evaluating load shedding and graceful degradation options, consider:
- Which metrics should you use to determine when shedding/GD should begin?
- What actions should be taken when the server is degraded?
- At what layer should shedding/GD be implemented? At every layer, or at a high-level choke point?

Code paths for graceful degradation and load shedding are probably not exercised often and hence can be a source of risk. There should be some mechanism to quickly disable load shedding and graceful degradation.

Retry with a jittered exponential backoff. Without jitter, a peturbation (e.g. a network blip) can cause many retries to be scheduled at the same time from different processes.

Use clear response codes to delineate retriable and non-retriable error conditions.

Requests should be tagged with a deadline; backends should short-circuit if there is no possibility that the deadline can be satisfied (e.g. if the request sat for too long in the queue). Deadlines should be set high in the stack (e.g. at the frontend) and propagated through the chain of backend services.

Latency cache vs. capacity cache: latency caches improve the latency of the service, but the service can still meet its capacity requirements even if the cache is cold; the service cannot meet its capacity requirements with a cold capacity cache.

"Always go downward in the stack", i.e. avoid intra-layer proxying. Intra-layer proxying can cause thread pool saturation to thread (e.g. proxying a request to a server with a saturated thread pool). Also, if the entire system is overloaded and a request is proxied due to a local error, the total load against the system only increases. If a client is communicating with the "wrong" server, rather than proxying the request, the server should advise the client on the correct server.
