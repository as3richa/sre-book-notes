---
title: "Handling Overload"
authors: ["Alejandro Forero Cuervo"]
---

One option for handling overloaded is to serve degraded responses (e.g. serving an old cached copy, searching only part of a corpus). However, under extreme overload, it may not even be possible to degrade gracefully, in which case errors must be served.

Queries per second and other static features of requests (e.g. "keys requested") are not necessarily a good proxy for load; even if, at some point in time, they perform adequately, the underlying ratios can change. A better solution is to measure capacity directly in e.g. CPU cores and memory. CPU consumption has been found to be a good signal for provisioning (in languages with a GC, memory pressure naturally translates to heightened CPU consumption; in languages without a GC, it's generally possible to overprovision memory, etc. such that CPU is the limiting factor).

Per-customer limits: in the case of overload, shed load by returning error responses only for customers that exceed pre-negotiated quotas.

Client-side throttling: if a client detects that significant fraction of its outgoing requests have yielded an "out of quota" error, it throttles itself, failing outgoing requests without ever hitting the network. Let requests and accepts with the number of requests and the number of accepted request in the last 2 minutes; then, the probability of a client failing a request locally is max(0, (requests - K * accepts) / (requests + 1)), where K is a parameter.

Criticality: requests are labelled with a one of four criticality values:
- CRITICAL_PLUS: failure has serious user-visible impact
- CRITICAL: default; failure has user-visible impact
- SHEDDABLE_PLUS: partial unavailability is expected
- SHEDDABLE: partial or full unavailability is unexpected

Labels are propagated in dependent requests to other services. Quotas and adaptive throttling take into account the criticality of individual requests. NB. criticality is orthogonal to latency requirements (e.g. search suggestions have stringent latency requirements but are highly sheddable).

Criticality is set as close as possible to browsers or mobile clients (e.g. in the HTTP frontends that actually render the page) and are rarely overridden deeper in the stack.

Utilizations signals are used on a per-task basis to reject requests as load approaches predefined limits. E.g. executor load average is the number of active threads, smoothed with exponential decay.

Two cases for overload within a given data center: a large subset of backend tasks are overloaded, or only a small subset are. The former case is mitigated by cross-datacenter load balancing, but if it occurs, the error should be propagated to the end user. In the latter case, a request should simply be retried by the client. Retries are treated no differently than new requests.

Clients implement a per-request retry budget: a single request is attempted only three times at most, after which the failure is allowed to propagate, since further attempts are likely to continue to fail. Clients also implement a per-client budget, in which a request is only retried if less than 10% of the client's outgoing requests are retries.

Each request from a client contains a metadata field indicating how many times the request has already been tried; backend tasks maintain a histogram of these values, and explicitly return a "don't retry" response if many requests are retries (because this indicates that other backend tasks are likely overloaded).

Large services tend to be deep stacks of many systems; retries should only occur immediately above the level that rejects a request. If the request does not succeed after retries, this layer returns a "don't retry" error in order to prevent previous layers of the stack from retrying recursively.

"A backend task provisioned to serve a certain traffic rate should continue to serve traffic at that rate without any significant impact on latency, regardless of how much excess traffic is thrown at the task."
